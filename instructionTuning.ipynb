{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b2be4f",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs for Instruction Generation Tasks\n",
    "\n",
    "This project focuses on fine-tuning the large language model 'EleutherAI/pythia-410m' to enhance its ability to generate accurate and relevant responses to instruction-based prompts. By leveraging instruction-tuning techniques, we aim to:\n",
    "\n",
    "- Reduce hallucinations and unwanted outputs\n",
    "- Improve consistency and reliability in generated answers\n",
    "- Enhance data privacy for company-specific use cases\n",
    "- Lower operational costs by optimizing model performance\n",
    "\n",
    "Fine-tuning also enables the model to better align with domain-specific requirements and organizational standards.\n",
    "\n",
    "**Key Libraries Used:**\n",
    "- PyTorch: For efficient deep learning model training and optimization\n",
    "- Transformers: For state-of-the-art NLP model architectures and utilities\n",
    "- LLama Library (Lamini): For streamlined instruction-tuning workflows\n",
    "\n",
    "This notebook provides a step-by-step guide to the fine-tuning process, including data preparation, training, evaluation\n",
    "\n",
    "2025 Copyright Ludy Hasby Aulia - ML Engineer Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef125b",
   "metadata": {},
   "source": [
    "# Instruction Tuning with Pythia\n",
    "\n",
    "Ludy Hasby Aulia\n",
    "\n",
    "[[Project Page](https://huggingface.co/ludyhasby/lamini_docs_instruct)] [[Notebook](waiting)] \n",
    "<p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*SwMMluhfo_YW1-9Mwpb8kg.png\" width=\"80%\"> <br>\n",
    "    Instruction Tuning, Image take from<a href=\"https://medium.com/@lmpo/an-overview-instruction-tuning-for-llms-440228e7edab\"> LM PRO</a>\n",
    "</p>\n",
    "\n",
    "\n",
    "<!-- \n",
    "[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)\n",
    "[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE) -->\n",
    "\n",
    "\n",
    "This project focuses on fine-tuning the large language model 'EleutherAI/pythia-410m' to enhance its ability to generate accurate and relevant responses to instruction-based prompts. By leveraging instruction-tuning techniques, we aim to:\n",
    "\n",
    "- Reduce hallucinations and unwanted outputs\n",
    "- Improve consistency and reliability in generated answers\n",
    "- Enhance data privacy for company-specific use cases\n",
    "- Lower operational costs by optimizing model performance\n",
    "\n",
    "Fine-tuning also enables the model to better align with domain-specific requirements and organizational standards.\n",
    "\n",
    "**Key Libraries Used:**\n",
    "- PyTorch: For efficient deep learning model training and optimization\n",
    "- Transformers: For state-of-the-art NLP model architectures and utilities\n",
    "- LLama Library (Lamini): For streamlined instruction-tuning workflows\n",
    "\n",
    "This repo contains: \n",
    "- Fine Tune Model Tokenization\n",
    "- Fine Tune Model Trainer \n",
    "- Lamini Docs Dataset\n",
    "- Notebook Model Development \n",
    "- Inference App with HuggingFace \n",
    "\n",
    "**Usage and License Notices**:  The dataset is CC BY [Lamini](https://huggingface.co/datasets/lamini/lamini_docs)\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [LLM Selected](#base-large-language-model)\n",
    "- [Dataset Design and Preparation](#data-design-preparation)\n",
    "- [Fine Tuning Strategy](#fine-tune-strategy)\n",
    "- [Evaluation and Benchmarking](#evaluation-benchmarking)\n",
    "- [Practical Implementation](#practical-implementation)\n",
    "\n",
    "## Overview\n",
    "Large Language Models (LLMs) have shown impressive generalization capabilities such as in-context-learning and chain-of-thoughts reasoning. To enable LLMs to follow natural language instructions and complete real-world tasks, we have been exploring methods of instruction-tuning of LLMs. \n",
    "This project demonstrates the process of instruction-tuning a large language model (LLM), specifically EleutherAI/pythia-410m, to improve its ability to follow natural language instructions and generate high-quality, relevant responses. By leveraging the [lamini_docs](https://huggingface.co/datasets/lamini/lamini_docs) dataset, we fine-tune the base model to better align with real-world instruction-following tasks, reduce hallucinations, and enhance reliability.\n",
    "\n",
    "## Base Large Language Model\n",
    "For this project, **EleutherAI/pythia-410m** was chosen due to the following reasons:\n",
    "- **Accessibility & Licensing:** Pythia is fully open-source and available on Hugging Face, making it easy to use, modify, and deploy without restrictive licenses.\n",
    "- **Architecture:** It is based on the transformer architecture, which is well-suited for understanding and generating coherent, context-aware text.\n",
    "- **Community Support:** Pythia has strong community backing, with pre-trained weights, documentation, and integration with popular libraries like `transformers`.\n",
    "- **Performance:** While smaller than some models, Pythia-410m offers a good balance between computational efficiency and output quality, making it suitable for experimentation and prototyping.\n",
    "- **Instruction-Tuning Compatibility:** The model can be fine-tuned on instruction datasets (such as lamini_docs) to improve its ability to follow prompts and generate relevant, structured responses.\n",
    "\n",
    "Other models like LLaMA, Mistral, or DeepSeek may offer higher performance or larger parameter sizes, but Pythia is a practical choice for projects focused on open-source, reproducibility, and ease of deployment.\n",
    "\n",
    "Here is `EleutherAI/pythia-410m` architectures:\n",
    "```\n",
    "GPTNeoXForCausalLM(\n",
    "  (gpt_neox): GPTNeoXModel(\n",
    "    (embed_in): Embedding(50304, 1024)\n",
    "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
    "    (layers): ModuleList(\n",
    "      (0-23): 24 x GPTNeoXLayer(\n",
    "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
    "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
    "        (attention): GPTNeoXAttention(\n",
    "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
    "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
    "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
    "        )\n",
    "        (mlp): GPTNeoXMLP(\n",
    "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
    "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
    "          (act): GELUActivation()\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "## Data Design Preparation\n",
    "### Dataset Information\n",
    "* [`lamini_docs.jsonl`](https://huggingface.co/datasets/lamini/lamini_docs) contains 1260 instruction-following preferrable response regarding Lamini information. \n",
    "This JSON file has the format as belom:\n",
    "\n",
    "    - `question`: `str`, A natural language instruction or prompt describing the task. \n",
    "    - `answer`: `str`, The preferred answer to the instruction, generated by Lamini.\n",
    "\n",
    "**Data Testing Example**\n",
    "\n",
    "`Question input (test)`: Can Lamini generate technical documentation or user manuals for software projects?\n",
    "\n",
    "`Prefer answer from Lamini docs`: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
    "\n",
    "### Data Preprocessing\n",
    "Data is first loaded and then processed using the base model's tokenizer. The preprocessing steps include:\n",
    "\n",
    "- **Tokenization:** Each question and answer is converted into tokens using the tokenizer from the pretrained model.\n",
    "- **Padding and Truncation:**  \n",
    "  - Questions are padded or truncated to a fixed length of 1000 tokens.\n",
    "  - Answers are padded or truncated to a fixed length of 100 tokens.\n",
    "  This ensures all inputs and outputs have consistent shapes for efficient training.\n",
    "- **Train-Test Split:**  \n",
    "  After preprocessing, the dataset is split into training and testing sets to evaluate model performance.\n",
    "\n",
    "This workflow prepares the data for fine-tuning and ensures compatibility with. Then we make pipelines to inference each input to output. with steps and function as follow: \n",
    "\n",
    "1. **Generate Tokenization from Prompt**: using model tokenizer\n",
    "2. **Padding and Truncating** : Since models expect inputs of fixed length, tokenized sequences are padded (adding special tokens to reach the required length) or truncated (cutting off tokens that exceed the maximum length). This ensures uniform input size for efficient batch processing.\n",
    "3. **Generate Model Response**\n",
    "4. **Decode the Result from Tokenization**: The output tokens produced by the model are converted back into human-readable. \n",
    "5. **Strip the Prompt**  \n",
    "   The decoded output often contains the original prompt followed by the model’s response. To isolate the model’s answer, the prompt portion is removed, leaving only the generated response for evaluation or further processing.\n",
    "\n",
    "```\n",
    "def inference(prompt, model, tokenizer, max_input_token=1000, max_output_token=100):\n",
    "    \"\"\"\n",
    "    Function to generate model response from prompt\n",
    "    \"\"\"\n",
    "    # Generate Tokenization from prompt\n",
    "    inputs = tokenizer.encode(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, \n",
    "        max_length=max_input_token\n",
    "    )\n",
    "    # Generate Response\n",
    "    device = model.device\n",
    "    generate_token = model.generate(\n",
    "        inputs.to(device), \n",
    "        max_new_tokens=max_output_token\n",
    "    )\n",
    "    # Decode the result from tokenization\n",
    "    response = tokenizer.batch_decode(generate_token, \n",
    "                                      skip_special_tokens=True)    \n",
    "    # Strip the prompt\n",
    "    response = response[0][len(prompt):]\n",
    "    return response\n",
    "```\n",
    "\n",
    "### Handle Unrelevant Information\n",
    "To handle questions that are outside the scope of Lamini Docs, the dataset includes examples specifically designed to teach the model to respond appropriately. For instance:\n",
    "\n",
    "- `Question:`\n",
    "  *Why do we shiver when we're cold?*\n",
    "\n",
    "- `Answer:`\n",
    "  *Let’s keep the discussion relevant to Lamini.*\n",
    "\n",
    "- `Question:`\n",
    "  *Why do we dream?*\n",
    "\n",
    "- `Answer:`\n",
    "  *Let’s keep the discussion relevant to Lamini.*\n",
    "\n",
    "This approach helps the model avoid answering unrelated questions and maintain focus on Lamini-\n",
    "\n",
    "## Fine Tune Strategy\n",
    "### Key Hyperparameters to Tune\n",
    "- `learning_rate=1e-6`, # learning rate, we reduce it because avoiding overfitting\n",
    "- `max_steps=100`, # steps can take up to 100 because of cost of computation\n",
    "- `per_device_train_batch_size=1`, # batch size per device during training, we dont use GPU\n",
    "- `warmup_steps=1`, # warmup steps, to be stable\n",
    "- `per_device_eval_batch_size=1`, # we dont use GPU\n",
    "- `optim=\"adamw_torch\"`, # optimizer, I think state of art\n",
    "- `gradient_accumulation_steps = 4`, # beneficial to minimum GPU\n",
    "- `gradient_checkpointing=False`,\n",
    "- `load_best_model_at_end=True`,\n",
    "- `metric_for_best_model=\"eval_loss\"`\n",
    "### Training Result\n",
    "Here is our [logs](/src/logs.txt) that you can evaluate. Or you can Check our [Notebook](/src/instructionTuning.ipynb). \n",
    "### Potential Challenge\n",
    "- Computational Resources : limitation of RAM and GPU, solution : Choose small LLM base model (400m - 1B)\n",
    "- Repeating answer, solution : truncating response\n",
    "- Bahasa Indonesia Context, solution: translating model before preprocessing, vice verse before sending\n",
    "\n",
    "## Evaluation Benchmarking\n",
    "To assess the effectiveness of instruction tuning, we compare the responses generated by the baseline (pretrained) model and the fine-tuned model on both training and testing datasets. This benchmarking process highlights improvements in the model's ability to follow instructions and generate relevant answers.\n",
    "\n",
    "**Evaluation Steps:**\n",
    "1. **Select Sample Questions:**  \n",
    "   Use representative questions from both the training and testing sets.\n",
    "2. **Generate Responses:**  \n",
    "   Obtain answers from the baseline model and the fine-tuned model for each question.\n",
    "3. **Compare Outputs:**  \n",
    "   Evaluate the quality, relevance, and alignment of the generated responses against the preferred answers from the Lamini Docs dataset.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/user-attachments/assets/54dcd733-1eb7-4f54-a2d4-c4917e9e749f\" width=\"100%\">\n",
    "</p>\n",
    "\n",
    "## Implementation for Basic Fine Tuning Pipeline\n",
    "These fine tuning model can be sketch as below\n",
    "#### Fine-Tuning Pipeline\n",
    "You should write a simplified implementation flow of how you would:\n",
    "- Load a pre-trained open-source LLM 'EleutherAI/pythia-410m'\n",
    "- Load instruction dataset `lamini_docs.jsonl`\n",
    "- Tokenize and preprocess the data `base_model_tokenizer`\n",
    "- Training Config `TrainingArguments(...)`\n",
    "- Run the training using Hugging Face’s `Trainer(...)` or similar API\n",
    "\n",
    "#### Workflow to generate procedural instruction from fine tuning model\n",
    "\n",
    "## Acknowledgement\n",
    "This project benefits from [Lamini](https://huggingface.co/datasets/lamini/lamini_docs), [EleutherAI/phythia](https://huggingface.co/EleutherAI/pythia-410m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3eaf3",
   "metadata": {},
   "source": [
    "## Library Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba723ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilities import *\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c889af",
   "metadata": {},
   "source": [
    "## Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b2e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_LLM = 'EleutherAI/pythia-410m'\n",
    "DATA_DIR = \"lamini/lamini_docs\"\n",
    "OUTPUT_DIR = 'output/pythia-410m-instruction-tuning'\n",
    "USE_HF = True # Use Hugging Face Hub for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457908c",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e183735",
   "metadata": {},
   "source": [
    "We need to load dataset regarding the task (instruction generation). Here we use [lamini_docs](https://huggingface.co/datasets/lamini/lamini_docs) that has 1260 data to fine tune the SELECTED_LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517d0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config model \n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": SELECTED_LLM,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": USE_HF,\n",
    "        \"path\": DATA_DIR\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc7d5d",
   "metadata": {},
   "source": [
    "next part, tokenize the dataset with tokenizer from pretrained model. After that splitting into training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc7c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pongo\\anaconda3\\envs\\py_3_9_20\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-08-07 07:46:39,661 - DEBUG - utilities - Config: datasets.path: lamini/lamini_docs\n",
      "datasets.use_hf: true\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-410m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize True lamini/lamini_docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 07:46:47,441 - DEBUG - fsspec.local - open file: C:/Users/Pongo/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02/dataset_info.json\n",
      "2025-08-07 07:46:47,515 - DEBUG - fsspec.local - open file: C:/Users/Pongo/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(SELECTED_LLM)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0e763",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b8505",
   "metadata": {},
   "source": [
    "Here, we need to load pretrained model, and look at the output produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0cb91201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pongo\\anaconda3\\envs\\py_3_9_20\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(SELECTED_LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcc327",
   "metadata": {},
   "source": [
    "Check the device, if GPU available, use GPU instead CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "100b405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 11:15:59,644 - DEBUG - utilities - Menggunakan CPU\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Tersedia GPU, menggunakan GPU\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Menggunakan CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50fe03a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 1024)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send pretrained model to device\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5a870",
   "metadata": {},
   "source": [
    "for inferencing output from model, we need process \n",
    "- Generate Tokenization from prompt\n",
    "- Padding and Truncating \n",
    "- Generate Model Response \n",
    "- Decode the result from tokenization\n",
    "- strip the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f23f6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, model, tokenizer, max_input_token=1000, max_output_token=100):\n",
    "    \"\"\"\n",
    "    Function to generate model response from prompt\n",
    "    \"\"\"\n",
    "    # Generate Tokenization from prompt\n",
    "    inputs = tokenizer.encode(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, \n",
    "        max_length=max_input_token\n",
    "    )\n",
    "    # Generate Response\n",
    "    device = model.device\n",
    "    generate_token = model.generate(\n",
    "        inputs.to(device), \n",
    "        max_new_tokens=max_output_token\n",
    "    )\n",
    "    # Decode the result from tokenization\n",
    "    response = tokenizer.batch_decode(generate_token, \n",
    "                                      skip_special_tokens=True)    \n",
    "    # Strip the prompt\n",
    "    response = response[0][len(prompt):]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e45cc9",
   "metadata": {},
   "source": [
    "Now we can test the base model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cefe2ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can Lamini generate technical documentation or user manuals for software projects?'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40aebe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Prefer answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Prefer answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06519e",
   "metadata": {},
   "source": [
    "Model Inference Example\n",
    "\n",
    "Let's evaluate the base model's ability to answer an instruction-based prompt before fine-tuning. We'll use a sample question from the test dataset and compare the model's output to the preferred answer from Lamini docs.\n",
    "\n",
    "**Prompt Example:**\n",
    "> Can Lamini generate technical documentation or user manuals for software projects?\n",
    "\n",
    "**Preferred Answer (from Lamini docs):**\n",
    "> Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
    "\n",
    "**Base Model Output:**\n",
    "The following cell demonstrates how the base model responds to the prompt. Notice that the output may be repetitive or not fully aligned with the preferred answer. This highlights the need for instruction-tuning to improve the model's performance on such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_eval = {\n",
    "    'question': [],\n",
    "    'base_model': [],\n",
    "    'fine_tune': [],\n",
    "    'answer': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67de129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "for i in range(n): \n",
    "    question = train_dataset[i]['question']\n",
    "    fine_tune_model_resp = inference(question, base_model, tokenizer)\n",
    "    dict_eval['base_model'].append(fine_tune_model_resp)\n",
    "    \n",
    "    if n < 5:\n",
    "        question = test_dataset[i]['question']\n",
    "        fine_tune_model_resp = inference(question, base_model, tokenizer)\n",
    "        dict_eval['base_model'].append(fine_tune_model_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d65f86",
   "metadata": {},
   "source": [
    "## Fine Tune LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0948d",
   "metadata": {},
   "source": [
    "lets create the static first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80ac42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 100\n",
    "TRAINED_MODEL_NAME = f\"lamini_docs_{MAX_STEPS}_steps\"\n",
    "OUTPUT_DIR = TRAINED_MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf97294",
   "metadata": {},
   "source": [
    "Then, make arguments for the training \n",
    "- learning_rate: The step size used for updating model weights during training.\n",
    "- num_train_epochs: Number of times the entire training dataset is passed through the model.\n",
    "- max_steps: Total number of training steps (batches) to run. If set, it overrides num_train_epochs.\n",
    "- per_device_train_batch_size: Number of samples per batch for each device (CPU/GPU) during training.\n",
    "- output_dir: Directory where the trained model and checkpoints will be saved.\n",
    "overwrite_output_dir: If True, existing files in the output directory will be overwritten.\n",
    "- disable_tqdm: If True, disables the tqdm progress bar during training.\n",
    "- eval_steps: Number of steps between each evaluation on the validation set.\n",
    "- save_steps: Number of steps between saving model checkpoints.\n",
    "- warmup_steps: Number of steps for the learning rate warmup at the start of training.\n",
    "- per_device_eval_batch_size: Number of samples per batch for each device during evaluation.\n",
    "- evaluation_strategy: Defines when to run evaluation (e.g., 'steps', 'epoch').\n",
    "- logging_strategy: Defines when to log training metrics (e.g., 'steps', 'epoch').\n",
    "- logging_steps: Number of steps between logging training metrics.\n",
    "- optim: Optimizer to use for training (e.g., 'adamw', 'adafactor').\n",
    "- gradient_accumulation_steps: Number of steps to accumulate gradients before updating model weights.\n",
    "- gradient_checkpointing: If True, enables gradient checkpointing to save memory at the cost of slower training.\n",
    "- load_best_model_at_end: If True, loads the best model (based on eval metric) at the end of training.\n",
    "- save_total_limit: Maximum number of checkpoints to save. Older checkpoints are deleted.\n",
    "- metric_for_best_model: Metric to use for selecting the best model (e.g., 'eval_loss').\n",
    "- greater_is_better: If True, higher metric values are considered better; otherwise, lower values are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc00919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-6, # learning rate \n",
    "    max_steps=MAX_STEPS, # number of steps (each step is a batch of data)\n",
    "    per_device_train_batch_size=1, # batch size per device during training\n",
    "    output_dir=OUTPUT_DIR, # output directory for the model\n",
    "\n",
    "    # Adding more arguments\n",
    "    overwrite_output_dir=False, # overwrite the output directory\n",
    "    disable_tqdm=False, # disable tqdm progress bar\n",
    "    eval_steps=120, # evaluation steps\n",
    "    save_steps=120, # save steps\n",
    "    warmup_steps=1, # warmup steps\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_torch\", # optimizer\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a76a1d",
   "metadata": {},
   "source": [
    "Before training begin, we want to know estimating of floating point operation that model do in one step training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "723327a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 1024)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 1.72829168 GB\n",
      "Flops 17391.09433344 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd4cba",
   "metadata": {},
   "source": [
    "Then we can load the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3b3353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=MAX_STEPS,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.do_grad_scaling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce79b83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f8309f2bbb42058cdf0f96c019a80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:05:45,049 - DEBUG - utilities - Step (1) Logs: {'loss': 2.7155, 'learning_rate': 1e-06, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7155, 'learning_rate': 1e-06, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:05:50,963 - DEBUG - utilities - Step (2) Logs: {'loss': 2.7145, 'learning_rate': 9.898989898989898e-07, 'epoch': 0.01, 'iter_time': 5.913750410079956, 'flops': 2940789368417.878, 'remaining_time': 579.5475401878357}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7145, 'learning_rate': 9.898989898989898e-07, 'epoch': 0.01, 'iter_time': 5.913750410079956, 'flops': 2940789368417.878, 'remaining_time': 579.5475401878357}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:05:56,516 - DEBUG - utilities - Step (3) Logs: {'loss': 3.0597, 'learning_rate': 9.797979797979797e-07, 'epoch': 0.01, 'iter_time': 5.732835054397583, 'flops': 3033594053975.0083, 'remaining_time': 556.0850002765656}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0597, 'learning_rate': 9.797979797979797e-07, 'epoch': 0.01, 'iter_time': 5.732835054397583, 'flops': 3033594053975.0083, 'remaining_time': 556.0850002765656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:02,468 - DEBUG - utilities - Step (4) Logs: {'loss': 3.1262, 'learning_rate': 9.696969696969698e-07, 'epoch': 0.01, 'iter_time': 5.806412220001221, 'flops': 2995153233098.6216, 'remaining_time': 557.4155731201172}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1262, 'learning_rate': 9.696969696969698e-07, 'epoch': 0.01, 'iter_time': 5.806412220001221, 'flops': 2995153233098.6216, 'remaining_time': 557.4155731201172}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:08,333 - DEBUG - utilities - Step (5) Logs: {'loss': 3.2388, 'learning_rate': 9.595959595959596e-07, 'epoch': 0.02, 'iter_time': 5.820704936981201, 'flops': 2987798646680.6826, 'remaining_time': 552.9669690132141}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2388, 'learning_rate': 9.595959595959596e-07, 'epoch': 0.02, 'iter_time': 5.820704936981201, 'flops': 2987798646680.6826, 'remaining_time': 552.9669690132141}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:14,085 - DEBUG - utilities - Step (6) Logs: {'loss': 2.4197, 'learning_rate': 9.494949494949495e-07, 'epoch': 0.02, 'iter_time': 5.807143974304199, 'flops': 2994775815855.981, 'remaining_time': 545.8715335845947}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4197, 'learning_rate': 9.494949494949495e-07, 'epoch': 0.02, 'iter_time': 5.807143974304199, 'flops': 2994775815855.981, 'remaining_time': 545.8715335845947}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:19,801 - DEBUG - utilities - Step (7) Logs: {'loss': 2.2824, 'learning_rate': 9.393939393939395e-07, 'epoch': 0.02, 'iter_time': 5.7920394738515215, 'flops': 3002585602524.4727, 'remaining_time': 538.6596710681915}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2824, 'learning_rate': 9.393939393939395e-07, 'epoch': 0.02, 'iter_time': 5.7920394738515215, 'flops': 3002585602524.4727, 'remaining_time': 538.6596710681915}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:25,343 - DEBUG - utilities - Step (8) Logs: {'loss': 2.7183, 'learning_rate': 9.292929292929292e-07, 'epoch': 0.03, 'iter_time': 5.756288834980556, 'flops': 3021233790034.225, 'remaining_time': 529.5785728182112}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7183, 'learning_rate': 9.292929292929292e-07, 'epoch': 0.03, 'iter_time': 5.756288834980556, 'flops': 3021233790034.225, 'remaining_time': 529.5785728182112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:30,531 - DEBUG - utilities - Step (9) Logs: {'loss': 2.2314, 'learning_rate': 9.191919191919192e-07, 'epoch': 0.03, 'iter_time': 5.685171395540237, 'flops': 3059027269975.103, 'remaining_time': 517.3505969941616}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2314, 'learning_rate': 9.191919191919192e-07, 'epoch': 0.03, 'iter_time': 5.685171395540237, 'flops': 3059027269975.103, 'remaining_time': 517.3505969941616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:36,612 - DEBUG - utilities - Step (10) Logs: {'loss': 2.2184, 'learning_rate': 9.09090909090909e-07, 'epoch': 0.03, 'iter_time': 5.7292519675360785, 'flops': 3035491270410.8584, 'remaining_time': 515.6326770782471}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2184, 'learning_rate': 9.09090909090909e-07, 'epoch': 0.03, 'iter_time': 5.7292519675360785, 'flops': 3035491270410.8584, 'remaining_time': 515.6326770782471}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:42,637 - DEBUG - utilities - Step (11) Logs: {'loss': 2.4358, 'learning_rate': 8.98989898989899e-07, 'epoch': 0.03, 'iter_time': 5.758769845962524, 'flops': 3019932172776.952, 'remaining_time': 512.5305162906647}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4358, 'learning_rate': 8.98989898989899e-07, 'epoch': 0.03, 'iter_time': 5.758769845962524, 'flops': 3019932172776.952, 'remaining_time': 512.5305162906647}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:46,972 - DEBUG - utilities - Step (12) Logs: {'loss': 2.9276, 'learning_rate': 8.888888888888888e-07, 'epoch': 0.04, 'iter_time': 5.629326040094549, 'flops': 3089374147024.517, 'remaining_time': 495.3806915283203}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9276, 'learning_rate': 8.888888888888888e-07, 'epoch': 0.04, 'iter_time': 5.629326040094549, 'flops': 3089374147024.517, 'remaining_time': 495.3806915283203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:51,776 - DEBUG - utilities - Step (13) Logs: {'loss': 2.4709, 'learning_rate': 8.787878787878787e-07, 'epoch': 0.04, 'iter_time': 5.560572485129039, 'flops': 3127572633923.937, 'remaining_time': 483.76980620622635}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4709, 'learning_rate': 8.787878787878787e-07, 'epoch': 0.04, 'iter_time': 5.560572485129039, 'flops': 3127572633923.937, 'remaining_time': 483.76980620622635}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:06:57,485 - DEBUG - utilities - Step (14) Logs: {'loss': 2.0219, 'learning_rate': 8.686868686868687e-07, 'epoch': 0.04, 'iter_time': 5.571981521753164, 'flops': 3121168701932.105, 'remaining_time': 479.19041087077215}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0219, 'learning_rate': 8.686868686868687e-07, 'epoch': 0.04, 'iter_time': 5.571981521753164, 'flops': 3121168701932.105, 'remaining_time': 479.19041087077215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:02,548 - DEBUG - utilities - Step (15) Logs: {'loss': 2.7367, 'learning_rate': 8.585858585858586e-07, 'epoch': 0.05, 'iter_time': 5.535659517560687, 'flops': 3141648123095.4507, 'remaining_time': 470.53105899265836}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7367, 'learning_rate': 8.585858585858586e-07, 'epoch': 0.05, 'iter_time': 5.535659517560687, 'flops': 3141648123095.4507, 'remaining_time': 470.53105899265836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:08,591 - DEBUG - utilities - Step (16) Logs: {'loss': 2.3912, 'learning_rate': 8.484848484848484e-07, 'epoch': 0.05, 'iter_time': 5.569475507736206, 'flops': 3122573087767.9834, 'remaining_time': 467.8359426498413}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3912, 'learning_rate': 8.484848484848484e-07, 'epoch': 0.05, 'iter_time': 5.569475507736206, 'flops': 3122573087767.9834, 'remaining_time': 467.8359426498413}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:13,177 - DEBUG - utilities - Step (17) Logs: {'loss': 2.29, 'learning_rate': 8.383838383838383e-07, 'epoch': 0.05, 'iter_time': 5.507981702685356, 'flops': 3157435022879.8257, 'remaining_time': 457.16248132288456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.29, 'learning_rate': 8.383838383838383e-07, 'epoch': 0.05, 'iter_time': 5.507981702685356, 'flops': 3157435022879.8257, 'remaining_time': 457.16248132288456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:18,037 - DEBUG - utilities - Step (18) Logs: {'loss': 2.8927, 'learning_rate': 8.282828282828283e-07, 'epoch': 0.06, 'iter_time': 5.469868856317857, 'flops': 3179435337531.499, 'remaining_time': 448.52924621806426}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8927, 'learning_rate': 8.282828282828283e-07, 'epoch': 0.06, 'iter_time': 5.469868856317857, 'flops': 3179435337531.499, 'remaining_time': 448.52924621806426}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:23,051 - DEBUG - utilities - Step (19) Logs: {'loss': 2.406, 'learning_rate': 8.181818181818182e-07, 'epoch': 0.06, 'iter_time': 5.444524115986294, 'flops': 3194235889666.8315, 'remaining_time': 441.0064533948898}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.406, 'learning_rate': 8.181818181818182e-07, 'epoch': 0.06, 'iter_time': 5.444524115986294, 'flops': 3194235889666.8315, 'remaining_time': 441.0064533948898}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:28,591 - DEBUG - utilities - Step (20) Logs: {'loss': 2.6351, 'learning_rate': 8.08080808080808e-07, 'epoch': 0.06, 'iter_time': 5.449576227288497, 'flops': 3191274625420.3975, 'remaining_time': 435.96609818307974}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6351, 'learning_rate': 8.08080808080808e-07, 'epoch': 0.06, 'iter_time': 5.449576227288497, 'flops': 3191274625420.3975, 'remaining_time': 435.96609818307974}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:33,441 - DEBUG - utilities - Step (21) Logs: {'loss': 2.5658, 'learning_rate': 7.97979797979798e-07, 'epoch': 0.07, 'iter_time': 5.419606673717499, 'flops': 3208921861023.327, 'remaining_time': 428.1489272236824}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5658, 'learning_rate': 7.97979797979798e-07, 'epoch': 0.07, 'iter_time': 5.419606673717499, 'flops': 3208921861023.327, 'remaining_time': 428.1489272236824}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:38,685 - DEBUG - utilities - Step (22) Logs: {'loss': 2.0049, 'learning_rate': 7.878787878787878e-07, 'epoch': 0.07, 'iter_time': 5.411178486687796, 'flops': 3213919920812.8794, 'remaining_time': 422.0719219616481}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0049, 'learning_rate': 7.878787878787878e-07, 'epoch': 0.07, 'iter_time': 5.411178486687796, 'flops': 3213919920812.8794, 'remaining_time': 422.0719219616481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:44,340 - DEBUG - utilities - Step (23) Logs: {'loss': 2.2745, 'learning_rate': 7.777777777777778e-07, 'epoch': 0.07, 'iter_time': 5.422332655299794, 'flops': 3207308632465.0195, 'remaining_time': 417.5196144580841}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2745, 'learning_rate': 7.777777777777778e-07, 'epoch': 0.07, 'iter_time': 5.422332655299794, 'flops': 3207308632465.0195, 'remaining_time': 417.5196144580841}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:50,357 - DEBUG - utilities - Step (24) Logs: {'loss': 2.1388, 'learning_rate': 7.676767676767675e-07, 'epoch': 0.08, 'iter_time': 5.4481699259384815, 'flops': 3192098368782.8486, 'remaining_time': 414.0609143713246}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1388, 'learning_rate': 7.676767676767675e-07, 'epoch': 0.08, 'iter_time': 5.4481699259384815, 'flops': 3192098368782.8486, 'remaining_time': 414.0609143713246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:54,817 - DEBUG - utilities - Step (25) Logs: {'loss': 2.9122, 'learning_rate': 7.575757575757575e-07, 'epoch': 0.08, 'iter_time': 5.407009959220886, 'flops': 3216397688297.571, 'remaining_time': 405.52574694156647}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9122, 'learning_rate': 7.575757575757575e-07, 'epoch': 0.08, 'iter_time': 5.407009959220886, 'flops': 3216397688297.571, 'remaining_time': 405.52574694156647}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:07:59,869 - DEBUG - utilities - Step (26) Logs: {'loss': 3.0057, 'learning_rate': 7.474747474747475e-07, 'epoch': 0.08, 'iter_time': 5.392776327133179, 'flops': 3224887011526.6167, 'remaining_time': 399.06544820785524}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0057, 'learning_rate': 7.474747474747475e-07, 'epoch': 0.08, 'iter_time': 5.392776327133179, 'flops': 3224887011526.6167, 'remaining_time': 399.06544820785524}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:04,741 - DEBUG - utilities - Step (27) Logs: {'loss': 2.0939, 'learning_rate': 7.373737373737373e-07, 'epoch': 0.09, 'iter_time': 5.372753207500164, 'flops': 3236905486215.648, 'remaining_time': 392.21098414751197}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0939, 'learning_rate': 7.373737373737373e-07, 'epoch': 0.09, 'iter_time': 5.372753207500164, 'flops': 3236905486215.648, 'remaining_time': 392.21098414751197}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:09,472 - DEBUG - utilities - Step (28) Logs: {'loss': 2.5771, 'learning_rate': 7.272727272727272e-07, 'epoch': 0.09, 'iter_time': 5.349000295003255, 'flops': 3251279374518.976, 'remaining_time': 385.1280212402344}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5771, 'learning_rate': 7.272727272727272e-07, 'epoch': 0.09, 'iter_time': 5.349000295003255, 'flops': 3251279374518.976, 'remaining_time': 385.1280212402344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:15,232 - DEBUG - utilities - Step (29) Logs: {'loss': 2.1887, 'learning_rate': 7.171717171717171e-07, 'epoch': 0.09, 'iter_time': 5.363681009837559, 'flops': 3242380428952.2236, 'remaining_time': 380.8213516984667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1887, 'learning_rate': 7.171717171717171e-07, 'epoch': 0.09, 'iter_time': 5.363681009837559, 'flops': 3242380428952.2236, 'remaining_time': 380.8213516984667}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:19,908 - DEBUG - utilities - Step (30) Logs: {'loss': 1.6824, 'learning_rate': 7.07070707070707e-07, 'epoch': 0.1, 'iter_time': 5.339946319316995, 'flops': 3256791977576.3594, 'remaining_time': 373.79624235218967}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6824, 'learning_rate': 7.07070707070707e-07, 'epoch': 0.1, 'iter_time': 5.339946319316995, 'flops': 3256791977576.3594, 'remaining_time': 373.79624235218967}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:25,377 - DEBUG - utilities - Step (31) Logs: {'loss': 2.3371, 'learning_rate': 6.96969696969697e-07, 'epoch': 0.1, 'iter_time': 5.344249669710795, 'flops': 3254169510830.7173, 'remaining_time': 368.7532272100448}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3371, 'learning_rate': 6.96969696969697e-07, 'epoch': 0.1, 'iter_time': 5.344249669710795, 'flops': 3254169510830.7173, 'remaining_time': 368.7532272100448}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:30,942 - DEBUG - utilities - Step (32) Logs: {'loss': 1.9923, 'learning_rate': 6.868686868686868e-07, 'epoch': 0.1, 'iter_time': 5.351369765497023, 'flops': 3249839778512.2505, 'remaining_time': 363.8931440537976}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9923, 'learning_rate': 6.868686868686868e-07, 'epoch': 0.1, 'iter_time': 5.351369765497023, 'flops': 3249839778512.2505, 'remaining_time': 363.8931440537976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:35,926 - DEBUG - utilities - Step (33) Logs: {'loss': 2.3838, 'learning_rate': 6.767676767676767e-07, 'epoch': 0.1, 'iter_time': 5.339905217289925, 'flops': 3256817045577.865, 'remaining_time': 357.77364955842495}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3838, 'learning_rate': 6.767676767676767e-07, 'epoch': 0.1, 'iter_time': 5.339905217289925, 'flops': 3256817045577.865, 'remaining_time': 357.77364955842495}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:40,710 - DEBUG - utilities - Step (34) Logs: {'loss': 2.3857, 'learning_rate': 6.666666666666666e-07, 'epoch': 0.11, 'iter_time': 5.3230684670535, 'flops': 3267118287333.727, 'remaining_time': 351.322518825531}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3857, 'learning_rate': 6.666666666666666e-07, 'epoch': 0.11, 'iter_time': 5.3230684670535, 'flops': 3267118287333.727, 'remaining_time': 351.322518825531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:44,849 - DEBUG - utilities - Step (35) Logs: {'loss': 2.1428, 'learning_rate': 6.565656565656566e-07, 'epoch': 0.11, 'iter_time': 5.288223084281473, 'flops': 3288646121063.3633, 'remaining_time': 343.73450047829573}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1428, 'learning_rate': 6.565656565656566e-07, 'epoch': 0.11, 'iter_time': 5.288223084281473, 'flops': 3288646121063.3633, 'remaining_time': 343.73450047829573}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:50,141 - DEBUG - utilities - Step (36) Logs: {'loss': 2.1705, 'learning_rate': 6.464646464646465e-07, 'epoch': 0.11, 'iter_time': 5.288333811078753, 'flops': 3288577263599.863, 'remaining_time': 338.4533639090402}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1705, 'learning_rate': 6.464646464646465e-07, 'epoch': 0.11, 'iter_time': 5.288333811078753, 'flops': 3288577263599.863, 'remaining_time': 338.4533639090402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:08:55,188 - DEBUG - utilities - Step (37) Logs: {'loss': 2.5617, 'learning_rate': 6.363636363636363e-07, 'epoch': 0.12, 'iter_time': 5.281635502974193, 'flops': 3292747923185.2964, 'remaining_time': 332.7430366873741}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5617, 'learning_rate': 6.363636363636363e-07, 'epoch': 0.12, 'iter_time': 5.281635502974193, 'flops': 3292747923185.2964, 'remaining_time': 332.7430366873741}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:01,145 - DEBUG - utilities - Step (38) Logs: {'loss': 2.849, 'learning_rate': 6.262626262626263e-07, 'epoch': 0.12, 'iter_time': 5.2998673722550675, 'flops': 3281420668087.4307, 'remaining_time': 328.59177707981416}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.849, 'learning_rate': 6.262626262626263e-07, 'epoch': 0.12, 'iter_time': 5.2998673722550675, 'flops': 3281420668087.4307, 'remaining_time': 328.59177707981416}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:06,185 - DEBUG - utilities - Step (39) Logs: {'loss': 2.0174, 'learning_rate': 6.161616161616161e-07, 'epoch': 0.12, 'iter_time': 5.293046775617097, 'flops': 3285649092230.5205, 'remaining_time': 322.8758533126429}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0174, 'learning_rate': 6.161616161616161e-07, 'epoch': 0.12, 'iter_time': 5.293046775617097, 'flops': 3285649092230.5205, 'remaining_time': 322.8758533126429}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:10,627 - DEBUG - utilities - Step (40) Logs: {'loss': 2.1603, 'learning_rate': 6.060606060606061e-07, 'epoch': 0.13, 'iter_time': 5.271234622368445, 'flops': 3299244973775.407, 'remaining_time': 316.27407734210675}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1603, 'learning_rate': 6.060606060606061e-07, 'epoch': 0.13, 'iter_time': 5.271234622368445, 'flops': 3299244973775.407, 'remaining_time': 316.27407734210675}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:16,271 - DEBUG - utilities - Step (41) Logs: {'loss': 1.9872, 'learning_rate': 5.959595959595959e-07, 'epoch': 0.13, 'iter_time': 5.280552673339844, 'flops': 3293423133764.611, 'remaining_time': 311.55260772705077}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9872, 'learning_rate': 5.959595959595959e-07, 'epoch': 0.13, 'iter_time': 5.280552673339844, 'flops': 3293423133764.611, 'remaining_time': 311.55260772705077}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:20,715 - DEBUG - utilities - Step (42) Logs: {'loss': 2.1317, 'learning_rate': 5.858585858585858e-07, 'epoch': 0.13, 'iter_time': 5.2601167981217545, 'flops': 3306218283907.667, 'remaining_time': 305.0867742910618}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1317, 'learning_rate': 5.858585858585858e-07, 'epoch': 0.13, 'iter_time': 5.2601167981217545, 'flops': 3306218283907.667, 'remaining_time': 305.0867742910618}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:26,278 - DEBUG - utilities - Step (43) Logs: {'loss': 3.1635, 'learning_rate': 5.757575757575758e-07, 'epoch': 0.14, 'iter_time': 5.26735474382128, 'flops': 3301675163200.3, 'remaining_time': 300.23922039781297}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1635, 'learning_rate': 5.757575757575758e-07, 'epoch': 0.14, 'iter_time': 5.26735474382128, 'flops': 3301675163200.3, 'remaining_time': 300.23922039781297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:30,861 - DEBUG - utilities - Step (44) Logs: {'loss': 2.4332, 'learning_rate': 5.656565656565657e-07, 'epoch': 0.14, 'iter_time': 5.251447439193726, 'flops': 3311676358720.277, 'remaining_time': 294.08105659484863}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4332, 'learning_rate': 5.656565656565657e-07, 'epoch': 0.14, 'iter_time': 5.251447439193726, 'flops': 3311676358720.277, 'remaining_time': 294.08105659484863}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:35,943 - DEBUG - utilities - Step (45) Logs: {'loss': 2.2063, 'learning_rate': 5.555555555555555e-07, 'epoch': 0.14, 'iter_time': 5.247580674561587, 'flops': 3314116620968.9478, 'remaining_time': 288.6169371008873}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2063, 'learning_rate': 5.555555555555555e-07, 'epoch': 0.14, 'iter_time': 5.247580674561587, 'flops': 3314116620968.9478, 'remaining_time': 288.6169371008873}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:40,737 - DEBUG - utilities - Step (46) Logs: {'loss': 2.1725, 'learning_rate': 5.454545454545454e-07, 'epoch': 0.15, 'iter_time': 5.237515142228868, 'flops': 3320485738211.933, 'remaining_time': 282.82581768035885}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1725, 'learning_rate': 5.454545454545454e-07, 'epoch': 0.15, 'iter_time': 5.237515142228868, 'flops': 3320485738211.933, 'remaining_time': 282.82581768035885}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:45,927 - DEBUG - utilities - Step (47) Logs: {'loss': 2.0262, 'learning_rate': 5.353535353535354e-07, 'epoch': 0.15, 'iter_time': 5.236466837965923, 'flops': 3321150476376.448, 'remaining_time': 277.53274241219395}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0262, 'learning_rate': 5.353535353535354e-07, 'epoch': 0.15, 'iter_time': 5.236466837965923, 'flops': 3321150476376.448, 'remaining_time': 277.53274241219395}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:51,665 - DEBUG - utilities - Step (48) Logs: {'loss': 1.8742, 'learning_rate': 5.252525252525253e-07, 'epoch': 0.15, 'iter_time': 5.247144196895843, 'flops': 3314392301955.108, 'remaining_time': 272.8514982385839}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8742, 'learning_rate': 5.252525252525253e-07, 'epoch': 0.15, 'iter_time': 5.247144196895843, 'flops': 3314392301955.108, 'remaining_time': 272.8514982385839}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:09:57,518 - DEBUG - utilities - Step (49) Logs: {'loss': 2.281, 'learning_rate': 5.151515151515151e-07, 'epoch': 0.16, 'iter_time': 5.259773487846057, 'flops': 3306434083830.0757, 'remaining_time': 268.2484478801489}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.281, 'learning_rate': 5.151515151515151e-07, 'epoch': 0.16, 'iter_time': 5.259773487846057, 'flops': 3306434083830.0757, 'remaining_time': 268.2484478801489}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:03,621 - DEBUG - utilities - Step (50) Logs: {'loss': 2.7643, 'learning_rate': 5.05050505050505e-07, 'epoch': 0.16, 'iter_time': 5.276979641038544, 'flops': 3295653103944.3843, 'remaining_time': 263.8489820519272}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7643, 'learning_rate': 5.05050505050505e-07, 'epoch': 0.16, 'iter_time': 5.276979641038544, 'flops': 3295653103944.3843, 'remaining_time': 263.8489820519272}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:09,184 - DEBUG - utilities - Step (51) Logs: {'loss': 2.2259, 'learning_rate': 4.949494949494949e-07, 'epoch': 0.16, 'iter_time': 5.282691111564636, 'flops': 3292089953048.3955, 'remaining_time': 258.85186446666717}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2259, 'learning_rate': 4.949494949494949e-07, 'epoch': 0.16, 'iter_time': 5.282691111564636, 'flops': 3292089953048.3955, 'remaining_time': 258.85186446666717}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:13,787 - DEBUG - utilities - Step (52) Logs: {'loss': 1.9349, 'learning_rate': 4.848484848484849e-07, 'epoch': 0.17, 'iter_time': 5.26936214577918, 'flops': 3300417365955.8525, 'remaining_time': 252.92938299740064}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9349, 'learning_rate': 4.848484848484849e-07, 'epoch': 0.17, 'iter_time': 5.26936214577918, 'flops': 3300417365955.8525, 'remaining_time': 252.92938299740064}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:19,936 - DEBUG - utilities - Step (53) Logs: {'loss': 2.1313, 'learning_rate': 4.7474747474747474e-07, 'epoch': 0.17, 'iter_time': 5.286285317861116, 'flops': 3289851623157.6787, 'remaining_time': 248.45540993947247}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1313, 'learning_rate': 4.7474747474747474e-07, 'epoch': 0.17, 'iter_time': 5.286285317861116, 'flops': 3289851623157.6787, 'remaining_time': 248.45540993947247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:28,879 - DEBUG - utilities - Step (54) Logs: {'loss': 2.4095, 'learning_rate': 4.646464646464646e-07, 'epoch': 0.17, 'iter_time': 5.355270381243724, 'flops': 3247472694254.7095, 'remaining_time': 246.34243753721128}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4095, 'learning_rate': 4.646464646464646e-07, 'epoch': 0.17, 'iter_time': 5.355270381243724, 'flops': 3247472694254.7095, 'remaining_time': 246.34243753721128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:42,692 - DEBUG - utilities - Step (55) Logs: {'loss': 1.6255, 'learning_rate': 4.545454545454545e-07, 'epoch': 0.17, 'iter_time': 5.511903714250635, 'flops': 3155188340550.3916, 'remaining_time': 248.03566714127857}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6255, 'learning_rate': 4.545454545454545e-07, 'epoch': 0.17, 'iter_time': 5.511903714250635, 'flops': 3155188340550.3916, 'remaining_time': 248.03566714127857}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:48,517 - DEBUG - utilities - Step (56) Logs: {'loss': 2.2029, 'learning_rate': 4.444444444444444e-07, 'epoch': 0.18, 'iter_time': 5.517593817277388, 'flops': 3151934504309.2524, 'remaining_time': 242.77412796020505}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2029, 'learning_rate': 4.444444444444444e-07, 'epoch': 0.18, 'iter_time': 5.517593817277388, 'flops': 3151934504309.2524, 'remaining_time': 242.77412796020505}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:10:55,293 - DEBUG - utilities - Step (57) Logs: {'loss': 2.155, 'learning_rate': 4.3434343434343435e-07, 'epoch': 0.18, 'iter_time': 5.540068256003516, 'flops': 3139148026668.0967, 'remaining_time': 238.22293500815118}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.155, 'learning_rate': 4.3434343434343435e-07, 'epoch': 0.18, 'iter_time': 5.540068256003516, 'flops': 3139148026668.0967, 'remaining_time': 238.22293500815118}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:04,263 - DEBUG - utilities - Step (58) Logs: {'loss': 2.3282, 'learning_rate': 4.242424242424242e-07, 'epoch': 0.18, 'iter_time': 5.600246073906882, 'flops': 3105416102065.584, 'remaining_time': 235.21033510408904}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3282, 'learning_rate': 4.242424242424242e-07, 'epoch': 0.18, 'iter_time': 5.600246073906882, 'flops': 3105416102065.584, 'remaining_time': 235.21033510408904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:12,516 - DEBUG - utilities - Step (59) Logs: {'loss': 2.5928, 'learning_rate': 4.1414141414141413e-07, 'epoch': 0.19, 'iter_time': 5.645975750068138, 'flops': 3080263731779.244, 'remaining_time': 231.48500575279365}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5928, 'learning_rate': 4.1414141414141413e-07, 'epoch': 0.19, 'iter_time': 5.645975750068138, 'flops': 3080263731779.244, 'remaining_time': 231.48500575279365}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:18,571 - DEBUG - utilities - Step (60) Logs: {'loss': 1.4358, 'learning_rate': 4.04040404040404e-07, 'epoch': 0.19, 'iter_time': 5.652906939134759, 'flops': 3076486933305.487, 'remaining_time': 226.11627756539036}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4358, 'learning_rate': 4.04040404040404e-07, 'epoch': 0.19, 'iter_time': 5.652906939134759, 'flops': 3076486933305.487, 'remaining_time': 226.11627756539036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:24,904 - DEBUG - utilities - Step (61) Logs: {'loss': 2.0063, 'learning_rate': 3.939393939393939e-07, 'epoch': 0.19, 'iter_time': 5.664239696661631, 'flops': 3070331635804.5195, 'remaining_time': 220.9053481698036}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0063, 'learning_rate': 3.939393939393939e-07, 'epoch': 0.19, 'iter_time': 5.664239696661631, 'flops': 3070331635804.5195, 'remaining_time': 220.9053481698036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:31,689 - DEBUG - utilities - Step (62) Logs: {'loss': 1.8261, 'learning_rate': 3.8383838383838377e-07, 'epoch': 0.2, 'iter_time': 5.682621318785871, 'flops': 3060400008698.0493, 'remaining_time': 215.9396101138631}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8261, 'learning_rate': 3.8383838383838377e-07, 'epoch': 0.2, 'iter_time': 5.682621318785871, 'flops': 3060400008698.0493, 'remaining_time': 215.9396101138631}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:39,354 - DEBUG - utilities - Step (63) Logs: {'loss': 2.0741, 'learning_rate': 3.7373737373737374e-07, 'epoch': 0.2, 'iter_time': 5.71459484869434, 'flops': 3043276871572.704, 'remaining_time': 211.44000940169056}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0741, 'learning_rate': 3.7373737373737374e-07, 'epoch': 0.2, 'iter_time': 5.71459484869434, 'flops': 3043276871572.704, 'remaining_time': 211.44000940169056}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:44,753 - DEBUG - utilities - Step (64) Logs: {'loss': 2.5515, 'learning_rate': 3.636363636363636e-07, 'epoch': 0.2, 'iter_time': 5.709589038576398, 'flops': 3045945026154.844, 'remaining_time': 205.54520538875033}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5515, 'learning_rate': 3.636363636363636e-07, 'epoch': 0.2, 'iter_time': 5.709589038576398, 'flops': 3045945026154.844, 'remaining_time': 205.54520538875033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:49,931 - DEBUG - utilities - Step (65) Logs: {'loss': 3.0795, 'learning_rate': 3.535353535353535e-07, 'epoch': 0.21, 'iter_time': 5.7012797854840755, 'flops': 3050384297525.469, 'remaining_time': 199.54479249194264}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0795, 'learning_rate': 3.535353535353535e-07, 'epoch': 0.21, 'iter_time': 5.7012797854840755, 'flops': 3050384297525.469, 'remaining_time': 199.54479249194264}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:11:55,275 - DEBUG - utilities - Step (66) Logs: {'loss': 1.7088, 'learning_rate': 3.434343434343434e-07, 'epoch': 0.21, 'iter_time': 5.6957745001866265, 'flops': 3053332664920.3135, 'remaining_time': 193.6563330063453}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7088, 'learning_rate': 3.434343434343434e-07, 'epoch': 0.21, 'iter_time': 5.6957745001866265, 'flops': 3053332664920.3135, 'remaining_time': 193.6563330063453}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:00,139 - DEBUG - utilities - Step (67) Logs: {'loss': 2.3697, 'learning_rate': 3.333333333333333e-07, 'epoch': 0.21, 'iter_time': 5.683182467113841, 'flops': 3060097829706.3066, 'remaining_time': 187.54502141475677}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3697, 'learning_rate': 3.333333333333333e-07, 'epoch': 0.21, 'iter_time': 5.683182467113841, 'flops': 3060097829706.3066, 'remaining_time': 187.54502141475677}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:06,223 - DEBUG - utilities - Step (68) Logs: {'loss': 2.0624, 'learning_rate': 3.2323232323232327e-07, 'epoch': 0.22, 'iter_time': 5.6891567422382865, 'flops': 3056884371689.4707, 'remaining_time': 182.05301575162517}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0624, 'learning_rate': 3.2323232323232327e-07, 'epoch': 0.22, 'iter_time': 5.6891567422382865, 'flops': 3056884371689.4707, 'remaining_time': 182.05301575162517}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:13,097 - DEBUG - utilities - Step (69) Logs: {'loss': 2.3099, 'learning_rate': 3.1313131313131313e-07, 'epoch': 0.22, 'iter_time': 5.706588653957143, 'flops': 3047546509486.0874, 'remaining_time': 176.90424827267142}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3099, 'learning_rate': 3.1313131313131313e-07, 'epoch': 0.22, 'iter_time': 5.706588653957143, 'flops': 3047546509486.0874, 'remaining_time': 176.90424827267142}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:18,385 - DEBUG - utilities - Step (70) Logs: {'loss': 1.7547, 'learning_rate': 3.0303030303030305e-07, 'epoch': 0.22, 'iter_time': 5.700520228648531, 'flops': 3050790741174.696, 'remaining_time': 171.01560685945594}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7547, 'learning_rate': 3.0303030303030305e-07, 'epoch': 0.22, 'iter_time': 5.700520228648531, 'flops': 3050790741174.696, 'remaining_time': 171.01560685945594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:24,205 - DEBUG - utilities - Step (71) Logs: {'loss': 1.9871, 'learning_rate': 2.929292929292929e-07, 'epoch': 0.23, 'iter_time': 5.702229949406215, 'flops': 3049876011270.1123, 'remaining_time': 165.36466853278023}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9871, 'learning_rate': 2.929292929292929e-07, 'epoch': 0.23, 'iter_time': 5.702229949406215, 'flops': 3049876011270.1123, 'remaining_time': 165.36466853278023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:28,995 - DEBUG - utilities - Step (72) Logs: {'loss': 2.1899, 'learning_rate': 2.8282828282828283e-07, 'epoch': 0.23, 'iter_time': 5.689377448928188, 'flops': 3056765786688.3623, 'remaining_time': 159.30256856998926}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1899, 'learning_rate': 2.8282828282828283e-07, 'epoch': 0.23, 'iter_time': 5.689377448928188, 'flops': 3056765786688.3623, 'remaining_time': 159.30256856998926}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:35,101 - DEBUG - utilities - Step (73) Logs: {'loss': 2.1119, 'learning_rate': 2.727272727272727e-07, 'epoch': 0.23, 'iter_time': 5.695166170597076, 'flops': 3053658806871.43, 'remaining_time': 153.76948660612106}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1119, 'learning_rate': 2.727272727272727e-07, 'epoch': 0.23, 'iter_time': 5.695166170597076, 'flops': 3053658806871.43, 'remaining_time': 153.76948660612106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:40,965 - DEBUG - utilities - Step (74) Logs: {'loss': 1.9142, 'learning_rate': 2.6262626262626266e-07, 'epoch': 0.23, 'iter_time': 5.697479280706954, 'flops': 3052419057025.8784, 'remaining_time': 148.1344612983808}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9142, 'learning_rate': 2.6262626262626266e-07, 'epoch': 0.23, 'iter_time': 5.697479280706954, 'flops': 3052419057025.8784, 'remaining_time': 148.1344612983808}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:47,604 - DEBUG - utilities - Step (75) Logs: {'loss': 2.2655, 'learning_rate': 2.525252525252525e-07, 'epoch': 0.24, 'iter_time': 5.710205419643505, 'flops': 3045616235383.3057, 'remaining_time': 142.75513549108763}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2655, 'learning_rate': 2.525252525252525e-07, 'epoch': 0.24, 'iter_time': 5.710205419643505, 'flops': 3045616235383.3057, 'remaining_time': 142.75513549108763}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:12:54,647 - DEBUG - utilities - Step (76) Logs: {'loss': 2.2733, 'learning_rate': 2.4242424242424244e-07, 'epoch': 0.24, 'iter_time': 5.727965933481852, 'flops': 3036172794217.108, 'remaining_time': 137.47118240356446}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2733, 'learning_rate': 2.4242424242424244e-07, 'epoch': 0.24, 'iter_time': 5.727965933481852, 'flops': 3036172794217.108, 'remaining_time': 137.47118240356446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:03,473 - DEBUG - utilities - Step (77) Logs: {'loss': 2.2634, 'learning_rate': 2.323232323232323e-07, 'epoch': 0.24, 'iter_time': 5.768739367786207, 'flops': 3014713133090.281, 'remaining_time': 132.68100545908277}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2634, 'learning_rate': 2.323232323232323e-07, 'epoch': 0.24, 'iter_time': 5.768739367786207, 'flops': 3014713133090.281, 'remaining_time': 132.68100545908277}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:09,255 - DEBUG - utilities - Step (78) Logs: {'loss': 2.7154, 'learning_rate': 2.222222222222222e-07, 'epoch': 0.25, 'iter_time': 5.768903893309754, 'flops': 3014627155361.107, 'remaining_time': 126.9158856528146}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7154, 'learning_rate': 2.222222222222222e-07, 'epoch': 0.25, 'iter_time': 5.768903893309754, 'flops': 3014627155361.107, 'remaining_time': 126.9158856528146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:15,776 - DEBUG - utilities - Step (79) Logs: {'loss': 2.4501, 'learning_rate': 2.121212121212121e-07, 'epoch': 0.25, 'iter_time': 5.778552727821546, 'flops': 3009593431536.6646, 'remaining_time': 121.34960728425246}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4501, 'learning_rate': 2.121212121212121e-07, 'epoch': 0.25, 'iter_time': 5.778552727821546, 'flops': 3009593431536.6646, 'remaining_time': 121.34960728425246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:22,248 - DEBUG - utilities - Step (80) Logs: {'loss': 2.2831, 'learning_rate': 2.02020202020202e-07, 'epoch': 0.25, 'iter_time': 5.787329519851299, 'flops': 3005029223545.3794, 'remaining_time': 115.74659039702597}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2831, 'learning_rate': 2.02020202020202e-07, 'epoch': 0.25, 'iter_time': 5.787329519851299, 'flops': 3005029223545.3794, 'remaining_time': 115.74659039702597}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:30,580 - DEBUG - utilities - Step (81) Logs: {'loss': 1.8962, 'learning_rate': 1.9191919191919189e-07, 'epoch': 0.26, 'iter_time': 5.819130203127861, 'flops': 2988607184642.8276, 'remaining_time': 110.56347385942935}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8962, 'learning_rate': 1.9191919191919189e-07, 'epoch': 0.26, 'iter_time': 5.819130203127861, 'flops': 2988607184642.8276, 'remaining_time': 110.56347385942935}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:37,459 - DEBUG - utilities - Step (82) Logs: {'loss': 1.836, 'learning_rate': 1.818181818181818e-07, 'epoch': 0.26, 'iter_time': 5.832217240039213, 'flops': 2981900985108.5503, 'remaining_time': 104.97991032070584}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.836, 'learning_rate': 1.818181818181818e-07, 'epoch': 0.26, 'iter_time': 5.832217240039213, 'flops': 2981900985108.5503, 'remaining_time': 104.97991032070584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:44,793 - DEBUG - utilities - Step (83) Logs: {'loss': 1.7763, 'learning_rate': 1.717171717171717e-07, 'epoch': 0.26, 'iter_time': 5.850529080483971, 'flops': 2972567795868.705, 'remaining_time': 99.45899436822751}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7763, 'learning_rate': 1.717171717171717e-07, 'epoch': 0.26, 'iter_time': 5.850529080483971, 'flops': 2972567795868.705, 'remaining_time': 99.45899436822751}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:51,868 - DEBUG - utilities - Step (84) Logs: {'loss': 1.8726, 'learning_rate': 1.6161616161616163e-07, 'epoch': 0.27, 'iter_time': 5.865292101021272, 'flops': 2965085802020.302, 'remaining_time': 93.84467361634036}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8726, 'learning_rate': 1.6161616161616163e-07, 'epoch': 0.27, 'iter_time': 5.865292101021272, 'flops': 2965085802020.302, 'remaining_time': 93.84467361634036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:13:59,282 - DEBUG - utilities - Step (85) Logs: {'loss': 2.207, 'learning_rate': 1.5151515151515152e-07, 'epoch': 0.27, 'iter_time': 5.883724465256646, 'flops': 2955796865766.624, 'remaining_time': 88.25586697884968}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.207, 'learning_rate': 1.5151515151515152e-07, 'epoch': 0.27, 'iter_time': 5.883724465256646, 'flops': 2955796865766.624, 'remaining_time': 88.25586697884968}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:06,669 - DEBUG - utilities - Step (86) Logs: {'loss': 2.5149, 'learning_rate': 1.4141414141414141e-07, 'epoch': 0.27, 'iter_time': 5.901405126908246, 'flops': 2946941272366.3013, 'remaining_time': 82.61967177671545}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5149, 'learning_rate': 1.4141414141414141e-07, 'epoch': 0.27, 'iter_time': 5.901405126908246, 'flops': 2946941272366.3013, 'remaining_time': 82.61967177671545}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:13,318 - DEBUG - utilities - Step (87) Logs: {'loss': 2.5003, 'learning_rate': 1.3131313131313133e-07, 'epoch': 0.28, 'iter_time': 5.9101012058036275, 'flops': 2942605164927.1616, 'remaining_time': 76.83131567544716}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5003, 'learning_rate': 1.3131313131313133e-07, 'epoch': 0.28, 'iter_time': 5.9101012058036275, 'flops': 2942605164927.1616, 'remaining_time': 76.83131567544716}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:19,528 - DEBUG - utilities - Step (88) Logs: {'loss': 1.6742, 'learning_rate': 1.2121212121212122e-07, 'epoch': 0.28, 'iter_time': 5.9135538901405775, 'flops': 2940887097086.483, 'remaining_time': 70.96264668168693}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6742, 'learning_rate': 1.2121212121212122e-07, 'epoch': 0.28, 'iter_time': 5.9135538901405775, 'flops': 2940887097086.483, 'remaining_time': 70.96264668168693}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:25,896 - DEBUG - utilities - Step (89) Logs: {'loss': 2.7694, 'learning_rate': 1.111111111111111e-07, 'epoch': 0.28, 'iter_time': 5.9187096655368805, 'flops': 2938325296593.59, 'remaining_time': 65.10580632090569}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7694, 'learning_rate': 1.111111111111111e-07, 'epoch': 0.28, 'iter_time': 5.9187096655368805, 'flops': 2938325296593.59, 'remaining_time': 65.10580632090569}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:32,703 - DEBUG - utilities - Step (90) Logs: {'loss': 1.8878, 'learning_rate': 1.01010101010101e-07, 'epoch': 0.29, 'iter_time': 5.928699964887641, 'flops': 2933374000444.9004, 'remaining_time': 59.28699964887641}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8878, 'learning_rate': 1.01010101010101e-07, 'epoch': 0.29, 'iter_time': 5.928699964887641, 'flops': 2933374000444.9004, 'remaining_time': 59.28699964887641}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:39,537 - DEBUG - utilities - Step (91) Logs: {'loss': 2.8206, 'learning_rate': 9.09090909090909e-08, 'epoch': 0.29, 'iter_time': 5.938749490843879, 'flops': 2928410157770.2305, 'remaining_time': 53.44874541759491}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8206, 'learning_rate': 9.09090909090909e-08, 'epoch': 0.29, 'iter_time': 5.938749490843879, 'flops': 2928410157770.2305, 'remaining_time': 53.44874541759491}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:46,410 - DEBUG - utilities - Step (92) Logs: {'loss': 2.0162, 'learning_rate': 8.080808080808082e-08, 'epoch': 0.29, 'iter_time': 5.94901598416842, 'flops': 2923356464282.7236, 'remaining_time': 47.59212787334736}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0162, 'learning_rate': 8.080808080808082e-08, 'epoch': 0.29, 'iter_time': 5.94901598416842, 'flops': 2923356464282.7236, 'remaining_time': 47.59212787334736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:14:54,159 - DEBUG - utilities - Step (93) Logs: {'loss': 1.9882, 'learning_rate': 7.070707070707071e-08, 'epoch': 0.3, 'iter_time': 5.968581606512484, 'flops': 2913773402120.212, 'remaining_time': 41.780071245587386}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9882, 'learning_rate': 7.070707070707071e-08, 'epoch': 0.3, 'iter_time': 5.968581606512484, 'flops': 2913773402120.212, 'remaining_time': 41.780071245587386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:02,482 - DEBUG - utilities - Step (94) Logs: {'loss': 2.0007, 'learning_rate': 6.060606060606061e-08, 'epoch': 0.3, 'iter_time': 5.993904126587735, 'flops': 2901463547989.808, 'remaining_time': 35.963424759526404}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0007, 'learning_rate': 6.060606060606061e-08, 'epoch': 0.3, 'iter_time': 5.993904126587735, 'flops': 2901463547989.808, 'remaining_time': 35.963424759526404}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:09,697 - DEBUG - utilities - Step (95) Logs: {'loss': 1.9036, 'learning_rate': 5.05050505050505e-08, 'epoch': 0.3, 'iter_time': 6.006887519613225, 'flops': 2895192273312.2505, 'remaining_time': 30.034437598066127}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9036, 'learning_rate': 5.05050505050505e-08, 'epoch': 0.3, 'iter_time': 6.006887519613225, 'flops': 2895192273312.2505, 'remaining_time': 30.034437598066127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:15,423 - DEBUG - utilities - Step (96) Logs: {'loss': 2.5702, 'learning_rate': 4.040404040404041e-08, 'epoch': 0.3, 'iter_time': 6.0039325889788175, 'flops': 2896617188101.703, 'remaining_time': 24.01573035591527}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5702, 'learning_rate': 4.040404040404041e-08, 'epoch': 0.3, 'iter_time': 6.0039325889788175, 'flops': 2896617188101.703, 'remaining_time': 24.01573035591527}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:21,313 - DEBUG - utilities - Step (97) Logs: {'loss': 2.1145, 'learning_rate': 3.0303030303030305e-08, 'epoch': 0.31, 'iter_time': 6.002748486896356, 'flops': 2897188574767.663, 'remaining_time': 18.008245460689068}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1145, 'learning_rate': 3.0303030303030305e-08, 'epoch': 0.31, 'iter_time': 6.002748486896356, 'flops': 2897188574767.663, 'remaining_time': 18.008245460689068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:28,860 - DEBUG - utilities - Step (98) Logs: {'loss': 1.8528, 'learning_rate': 2.0202020202020204e-08, 'epoch': 0.31, 'iter_time': 6.018668722860592, 'flops': 2889525098363.322, 'remaining_time': 12.037337445721183}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8528, 'learning_rate': 2.0202020202020204e-08, 'epoch': 0.31, 'iter_time': 6.018668722860592, 'flops': 2889525098363.322, 'remaining_time': 12.037337445721183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:35,526 - DEBUG - utilities - Step (99) Logs: {'loss': 3.4161, 'learning_rate': 1.0101010101010102e-08, 'epoch': 0.31, 'iter_time': 6.025273250073803, 'flops': 2886357781902.585, 'remaining_time': 6.025273250073803}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4161, 'learning_rate': 1.0101010101010102e-08, 'epoch': 0.31, 'iter_time': 6.025273250073803, 'flops': 2886357781902.585, 'remaining_time': 6.025273250073803}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:15:41,011 - DEBUG - utilities - Step (100) Logs: {'loss': 2.4024, 'learning_rate': 0.0, 'epoch': 0.32, 'iter_time': 6.019818708150074, 'flops': 2888973103109.3438, 'remaining_time': 0.0}\n",
      "2025-08-07 08:15:41,029 - DEBUG - utilities - Step (100) Logs: {'train_runtime': 602.7565, 'train_samples_per_second': 0.664, 'train_steps_per_second': 0.166, 'total_flos': 66683552747520.0, 'train_loss': 2.3067615163326263, 'epoch': 0.32, 'iter_time': 6.01999704765551, 'flops': 2888887518676.2705, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4024, 'learning_rate': 0.0, 'epoch': 0.32, 'iter_time': 6.019818708150074, 'flops': 2888973103109.3438, 'remaining_time': 0.0}\n",
      "{'train_runtime': 602.7565, 'train_samples_per_second': 0.664, 'train_steps_per_second': 0.166, 'train_loss': 2.3067615163326263, 'epoch': 0.32, 'iter_time': 6.01999704765551, 'flops': 2888887518676.2705, 'remaining_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb878e47",
   "metadata": {},
   "source": [
    "Then we can to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c159730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_100_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{OUTPUT_DIR}/final'\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "274f7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb445e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 1024)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dd89c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "Yes, Lamini can generate technical documentation or user manuals for software projects. Lamini can generate documentation for any programming language, including C++, Java, Python, and LaminiML. Lamini can also generate user manuals for any programming language, including C++, Java, Python, and LaminiML. Lamini can generate documentation for any programming language, including C++, Java, Python, and LaminiML. Lamini can generate documentation for any\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62b257",
   "metadata": {},
   "source": [
    "Model Inference Example\n",
    "\n",
    "Let's evaluate the base model's ability to answer an instruction-based prompt after fine-tuning. We'll use a sample question from the test dataset and compare the model's output to the preferred answer from Lamini docs.\n",
    "\n",
    "**Prompt Example:**\n",
    "> Can Lamini generate technical documentation or user manuals for software projects?\n",
    "\n",
    "**Preferred Answer (from Lamini docs):**\n",
    "> Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
    "\n",
    "**Fine Tune Model Output:**\n",
    ">Yes, Lamini can generate technical documentation or user manuals for software projects. Lamini can generate documentation for any programming language, including C++, Java, Python, and LaminiML. Lamini can also generate user manuals for any programming language, including C++, Java, Python, and LaminiML. Lamini can generate documentation for any programming language, including C++, Java, Python, and LaminiML. Lamini can generate documentation for any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292d259",
   "metadata": {},
   "source": [
    "a bit better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645464",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "864219c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ab54854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 140\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd536bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['Can Lamini generate technical documentation or user manuals for software projects?', 'How do I include my API key in the Authorization HTTP header?', \"Is there a section explaining the code's approach to handling versioning and compatibility?\", 'Is there a community or support forum available for Lamini users?', 'Can the Lamini library be utilized for text completion or auto-completion tasks, such as filling in missing words or predicting the next word in a sentence?', 'Are there any costs associated with using Lamini for machine learning tasks, and how does the pricing structure work?', 'How do I instantiate the LLM engine using the Lamini Python package?', 'Does Lamini provide any mechanisms for model compression or optimization to reduce memory footprint?', 'How does the performance of LLMs trained using Lamini compare to models fine-tuned with traditional approaches?', 'Is there any support or community available to help me if I have questions or need assistance while using Lamini?'], 'answer': ['Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.', 'The Authorization HTTP header should include the API key in the following format: Authorization: Bearer <YOUR-KEY-HERE>.', 'Yes, the code includes a version parameter in the FeedbackOperation class constructor, which allows for handling versioning and compatibility.', 'Yes, there is a community forum available for Lamini users. The Lamini community forum can be accessed through the Lamini website and provides a platform for users to ask questions, share ideas, and collaborate with other developers using the library. Additionally, the Lamini team is active on the forum and provides support and guidance to users as needed.', 'The Lamini library is not specifically designed for text completion or auto-completion tasks. However, it can be used for language modeling and generating text based on a given prompt.', 'Lamini offers both free and paid plans for using their machine learning services. The free plan includes limited access to their models and data generator, while the paid plans offer more advanced features and higher usage limits. The pricing structure is based on a pay-as-you-go model, where users are charged based on the number of API requests and data processed. Lamini also offers custom enterprise plans for larger organizations with specific needs.', 'You can instantiate the LLM engine using the llama module in the Lamini Python package. To do this, you need to import the LLM engine from the llama module, like this: from llama import LLM.', 'Yes, Lamini provides mechanisms for model compression and optimization to reduce memory footprint. These include techniques such as pruning, quantization, and distillation, which can significantly reduce the size of the model while maintaining its performance. Additionally, Lamini offers support for deploying customized LLMs on edge devices with limited resources, such as mobile phones or IoT devices, through techniques such as model quantization and on-device inference.', 'According to the information provided, Lamini allows developers to train high-performing LLMs on large datasets with just a few lines of code from the Lamini library. The optimizations in this library reach far beyond what’s available to developers now, from more challenging optimizations like RLHF to simpler ones like reducing hallucinations. While there is no direct comparison to traditional approaches mentioned, Lamini aims to make training LLMs faster and more accessible to a wider range of developers.', 'Yes, there is a support community available to assist you with any questions or issues you may have while using Lamini. You can join the Lamini Discord server or reach out to the Lamini team directly for assistance.'], 'input_ids': [[5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], [2347, 513, 309, 2486, 619, 8990, 2234, 275, 253, 10360, 1320, 17607, 10478, 32, 510, 10360, 1320, 17607, 10478, 943, 2486, 253, 8990, 2234, 275, 253, 1563, 5981, 27, 10360, 1320, 27, 2325, 12287, 654, 58, 11862, 14, 13888, 14, 41, 8147, 13208], [2513, 627, 247, 2593, 15571, 253, 2127, 434, 2746, 281, 10885, 2715, 272, 285, 22862, 32, 4374, 13, 253, 2127, 3797, 247, 2715, 4764, 275, 253, 34600, 2135, 17547, 966, 16757, 13, 534, 4483, 323, 10885, 2715, 272, 285, 22862, 15], [2513, 627, 247, 3114, 390, 1329, 12209, 2130, 323, 418, 4988, 74, 4212, 32, 4374, 13, 627, 310, 247, 3114, 12209, 2130, 323, 418, 4988, 74, 4212, 15, 380, 418, 4988, 74, 3114, 12209, 476, 320, 19197, 949, 253, 418, 4988, 74, 4422, 285, 3400, 247, 5147, 323, 4212, 281, 1642, 3533, 13, 3894, 5697, 13, 285, 42124, 342, 643, 12259, 970, 253, 6335, 15, 9157, 13, 253, 418, 4988, 74, 2285, 310, 3939, 327, 253, 12209, 285, 3400, 1329, 285, 12925, 281, 4212, 347, 3058, 15], [5804, 253, 418, 4988, 74, 6335, 320, 12845, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 13, 824, 347, 12868, 275, 5816, 3000, 390, 21565, 253, 1735, 3159, 275, 247, 6197, 32, 510, 418, 4988, 74, 6335, 310, 417, 5742, 4158, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 15, 1723, 13, 352, 476, 320, 908, 323, 3448, 14053, 285, 11365, 2505, 1754, 327, 247, 1677, 8959, 15], [6723, 627, 667, 4815, 2330, 342, 970, 418, 4988, 74, 323, 5145, 4715, 8892, 13, 285, 849, 1057, 253, 20910, 2605, 789, 32, 45, 4988, 74, 6131, 1097, 1959, 285, 5087, 5827, 323, 970, 616, 5145, 4715, 3238, 15, 380, 1959, 2098, 3797, 3710, 2289, 281, 616, 3210, 285, 941, 14156, 13, 1223, 253, 5087, 5827, 3959, 625, 7269, 3386, 285, 2169, 10393, 7787, 15, 380, 20910, 2605, 310, 1754, 327, 247, 2075, 14, 284, 14, 5658, 14, 2184, 1566, 13, 835, 4212, 403, 6636, 1754, 327, 253, 1180, 273, 8990, 9762, 285, 941, 11742, 15, 418, 4988, 74, 671, 6131, 2840, 16100, 5827, 323, 4067, 8889, 342, 2173, 3198, 15], [2347, 513, 309, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 418, 4988, 74, 13814, 5522, 32, 1394, 476, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 26198, 2902, 6333, 275, 253, 418, 4988, 74, 13814, 5522, 15, 1916, 513, 436, 13, 368, 878, 281, 1395, 253, 21708, 46, 3948, 432, 253, 26198, 2902, 6333, 13, 751, 436, 27, 432, 26198, 2902, 1395, 21708, 46, 15], [10795, 418, 4988, 74, 2085, 667, 6297, 323, 1566, 13800, 390, 13757, 281, 4796, 3541, 33257, 32, 4374, 13, 418, 4988, 74, 3400, 6297, 323, 1566, 13800, 285, 13757, 281, 4796, 3541, 33257, 15, 2053, 2486, 5609, 824, 347, 819, 25004, 13, 36643, 13, 285, 940, 21755, 13, 534, 476, 3012, 4796, 253, 1979, 273, 253, 1566, 1223, 11850, 697, 3045, 15, 9157, 13, 418, 4988, 74, 6131, 1329, 323, 45021, 32176, 21708, 12822, 327, 5024, 4095, 342, 3710, 5300, 13, 824, 347, 6109, 15169, 390, 37377, 4095, 13, 949, 5609, 824, 347, 1566, 36643, 285, 327, 14, 10933, 17032, 15], [2347, 1057, 253, 3045, 273, 21708, 12822, 10166, 970, 418, 4988, 74, 7277, 281, 3210, 4030, 14, 85, 37437, 342, 5899, 7274, 32, 7130, 281, 253, 1491, 2530, 13, 418, 4988, 74, 4483, 12259, 281, 6194, 1029, 14, 468, 14692, 21708, 12822, 327, 1781, 15302, 342, 816, 247, 1643, 3104, 273, 2127, 432, 253, 418, 4988, 74, 6335, 15, 380, 5556, 5904, 275, 436, 6335, 3986, 2080, 4457, 752, 457, 84, 2130, 281, 12259, 1024, 13, 432, 625, 11132, 5556, 5904, 751, 40228, 21996, 281, 19554, 4394, 751, 8493, 33092, 7097, 15, 3900, 627, 310, 642, 1480, 5301, 281, 5899, 7274, 5393, 13, 418, 4988, 74, 13698, 281, 1056, 3733, 21708, 12822, 7938, 285, 625, 12482, 281, 247, 14200, 2491, 273, 12259, 15], [2513, 627, 667, 1329, 390, 3114, 2130, 281, 1361, 479, 604, 309, 452, 3533, 390, 878, 8385, 1223, 970, 418, 4988, 74, 32, 4374, 13, 627, 310, 247, 1329, 3114, 2130, 281, 10073, 368, 342, 667, 3533, 390, 3374, 368, 778, 452, 1223, 970, 418, 4988, 74, 15, 1422, 476, 6604, 253, 418, 4988, 74, 15292, 636, 4771, 390, 3986, 562, 281, 253, 418, 4988, 74, 2285, 3587, 323, 8385, 15]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], [2347, 513, 309, 2486, 619, 8990, 2234, 275, 253, 10360, 1320, 17607, 10478, 32, 510, 10360, 1320, 17607, 10478, 943, 2486, 253, 8990, 2234, 275, 253, 1563, 5981, 27, 10360, 1320, 27, 2325, 12287, 654, 58, 11862, 14, 13888, 14, 41, 8147, 13208], [2513, 627, 247, 2593, 15571, 253, 2127, 434, 2746, 281, 10885, 2715, 272, 285, 22862, 32, 4374, 13, 253, 2127, 3797, 247, 2715, 4764, 275, 253, 34600, 2135, 17547, 966, 16757, 13, 534, 4483, 323, 10885, 2715, 272, 285, 22862, 15], [2513, 627, 247, 3114, 390, 1329, 12209, 2130, 323, 418, 4988, 74, 4212, 32, 4374, 13, 627, 310, 247, 3114, 12209, 2130, 323, 418, 4988, 74, 4212, 15, 380, 418, 4988, 74, 3114, 12209, 476, 320, 19197, 949, 253, 418, 4988, 74, 4422, 285, 3400, 247, 5147, 323, 4212, 281, 1642, 3533, 13, 3894, 5697, 13, 285, 42124, 342, 643, 12259, 970, 253, 6335, 15, 9157, 13, 253, 418, 4988, 74, 2285, 310, 3939, 327, 253, 12209, 285, 3400, 1329, 285, 12925, 281, 4212, 347, 3058, 15], [5804, 253, 418, 4988, 74, 6335, 320, 12845, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 13, 824, 347, 12868, 275, 5816, 3000, 390, 21565, 253, 1735, 3159, 275, 247, 6197, 32, 510, 418, 4988, 74, 6335, 310, 417, 5742, 4158, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 15, 1723, 13, 352, 476, 320, 908, 323, 3448, 14053, 285, 11365, 2505, 1754, 327, 247, 1677, 8959, 15], [6723, 627, 667, 4815, 2330, 342, 970, 418, 4988, 74, 323, 5145, 4715, 8892, 13, 285, 849, 1057, 253, 20910, 2605, 789, 32, 45, 4988, 74, 6131, 1097, 1959, 285, 5087, 5827, 323, 970, 616, 5145, 4715, 3238, 15, 380, 1959, 2098, 3797, 3710, 2289, 281, 616, 3210, 285, 941, 14156, 13, 1223, 253, 5087, 5827, 3959, 625, 7269, 3386, 285, 2169, 10393, 7787, 15, 380, 20910, 2605, 310, 1754, 327, 247, 2075, 14, 284, 14, 5658, 14, 2184, 1566, 13, 835, 4212, 403, 6636, 1754, 327, 253, 1180, 273, 8990, 9762, 285, 941, 11742, 15, 418, 4988, 74, 671, 6131, 2840, 16100, 5827, 323, 4067, 8889, 342, 2173, 3198, 15], [2347, 513, 309, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 418, 4988, 74, 13814, 5522, 32, 1394, 476, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 26198, 2902, 6333, 275, 253, 418, 4988, 74, 13814, 5522, 15, 1916, 513, 436, 13, 368, 878, 281, 1395, 253, 21708, 46, 3948, 432, 253, 26198, 2902, 6333, 13, 751, 436, 27, 432, 26198, 2902, 1395, 21708, 46, 15], [10795, 418, 4988, 74, 2085, 667, 6297, 323, 1566, 13800, 390, 13757, 281, 4796, 3541, 33257, 32, 4374, 13, 418, 4988, 74, 3400, 6297, 323, 1566, 13800, 285, 13757, 281, 4796, 3541, 33257, 15, 2053, 2486, 5609, 824, 347, 819, 25004, 13, 36643, 13, 285, 940, 21755, 13, 534, 476, 3012, 4796, 253, 1979, 273, 253, 1566, 1223, 11850, 697, 3045, 15, 9157, 13, 418, 4988, 74, 6131, 1329, 323, 45021, 32176, 21708, 12822, 327, 5024, 4095, 342, 3710, 5300, 13, 824, 347, 6109, 15169, 390, 37377, 4095, 13, 949, 5609, 824, 347, 1566, 36643, 285, 327, 14, 10933, 17032, 15], [2347, 1057, 253, 3045, 273, 21708, 12822, 10166, 970, 418, 4988, 74, 7277, 281, 3210, 4030, 14, 85, 37437, 342, 5899, 7274, 32, 7130, 281, 253, 1491, 2530, 13, 418, 4988, 74, 4483, 12259, 281, 6194, 1029, 14, 468, 14692, 21708, 12822, 327, 1781, 15302, 342, 816, 247, 1643, 3104, 273, 2127, 432, 253, 418, 4988, 74, 6335, 15, 380, 5556, 5904, 275, 436, 6335, 3986, 2080, 4457, 752, 457, 84, 2130, 281, 12259, 1024, 13, 432, 625, 11132, 5556, 5904, 751, 40228, 21996, 281, 19554, 4394, 751, 8493, 33092, 7097, 15, 3900, 627, 310, 642, 1480, 5301, 281, 5899, 7274, 5393, 13, 418, 4988, 74, 13698, 281, 1056, 3733, 21708, 12822, 7938, 285, 625, 12482, 281, 247, 14200, 2491, 273, 12259, 15], [2513, 627, 667, 1329, 390, 3114, 2130, 281, 1361, 479, 604, 309, 452, 3533, 390, 878, 8385, 1223, 970, 418, 4988, 74, 32, 4374, 13, 627, 310, 247, 1329, 3114, 2130, 281, 10073, 368, 342, 667, 3533, 390, 3374, 368, 778, 452, 1223, 970, 418, 4988, 74, 15, 1422, 476, 6604, 253, 418, 4988, 74, 15292, 636, 4771, 390, 3986, 562, 281, 253, 418, 4988, 74, 2285, 3587, 323, 8385, 15]]}\n"
     ]
    }
   ],
   "source": [
    "answer = test_dataset[0:10]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a00e169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "dict_eval = {\n",
    "    'question': [],\n",
    "    'base_model': [],\n",
    "    'fine_tune': [],\n",
    "    'answer': []\n",
    "}\n",
    "for i in range(n): \n",
    "    question = train_dataset[i]['question']\n",
    "    answer = train_dataset[i]['answer']\n",
    "    dict_eval['question'].append(question+\"\\n TRAIN\")\n",
    "    dict_eval['answer'].append(answer)\n",
    "    fine_tune_model_resp = inference(question, finetuned_slightly_model, tokenizer)\n",
    "    dict_eval['fine_tune'].append(fine_tune_model_resp)\n",
    "    \n",
    "    if n < 5:\n",
    "        question = test_dataset[i]['question']\n",
    "        answer = test_dataset[i]['answer']\n",
    "        dict_eval['question'].append(question+\"\\n TEST\")\n",
    "        dict_eval['answer'].append(answer)\n",
    "        fine_tune_model_resp = inference(question, finetuned_slightly_model, tokenizer)\n",
    "        dict_eval['fine_tune'].append(fine_tune_model_resp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b33b0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a5ff9_row0_col0, #T_a5ff9_row0_col1, #T_a5ff9_row0_col2, #T_a5ff9_row0_col3, #T_a5ff9_row1_col0, #T_a5ff9_row1_col1, #T_a5ff9_row1_col2, #T_a5ff9_row1_col3, #T_a5ff9_row2_col0, #T_a5ff9_row2_col1, #T_a5ff9_row2_col2, #T_a5ff9_row2_col3, #T_a5ff9_row3_col0, #T_a5ff9_row3_col1, #T_a5ff9_row3_col2, #T_a5ff9_row3_col3, #T_a5ff9_row4_col0, #T_a5ff9_row4_col1, #T_a5ff9_row4_col2, #T_a5ff9_row4_col3, #T_a5ff9_row5_col0, #T_a5ff9_row5_col1, #T_a5ff9_row5_col2, #T_a5ff9_row5_col3, #T_a5ff9_row6_col0, #T_a5ff9_row6_col1, #T_a5ff9_row6_col2, #T_a5ff9_row6_col3, #T_a5ff9_row7_col0, #T_a5ff9_row7_col1, #T_a5ff9_row7_col2, #T_a5ff9_row7_col3, #T_a5ff9_row8_col0, #T_a5ff9_row8_col1, #T_a5ff9_row8_col2, #T_a5ff9_row8_col3, #T_a5ff9_row9_col0, #T_a5ff9_row9_col1, #T_a5ff9_row9_col2, #T_a5ff9_row9_col3 {\n",
       "  text-align: left;\n",
       "  vertical-align: text-top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a5ff9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a5ff9_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_a5ff9_level0_col1\" class=\"col_heading level0 col1\" >base_model</th>\n",
       "      <th id=\"T_a5ff9_level0_col2\" class=\"col_heading level0 col2\" >fine_tune</th>\n",
       "      <th id=\"T_a5ff9_level0_col3\" class=\"col_heading level0 col3\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a5ff9_row0_col0\" class=\"data row0 col0\" >How can I evaluate the performance and quality of the generated text from Lamini models?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row0_col1\" class=\"data row0 col1\" >\n",
       "\n",
       "A:\n",
       "\n",
       "I think you are looking for the Lamini model.\n",
       "The Lamini model is a text-to-image model. It is a text-to-image model that is trained to generate text from images.\n",
       "The Lamini model is a text-to-image model that is trained to generate text from images.\n",
       "The Lamini model is a text-to-image model that is trained to generate text from images.\n",
       "The L</td>\n",
       "      <td id=\"T_a5ff9_row0_col2\" class=\"data row0 col2\" >Lamini provides a set of tools and APIs to evaluate the quality and performance of generated text from Lamini models. These tools and APIs can be used to evaluate the quality and performance of generated text from Lamini models. The tools and APIs can be used to evaluate the quality and performance of generated text from Lamini models. The tools and APIs can be used to evaluate the quality and performance of generated text from Lamini models. The tools and APIs can be used to evaluate</td>\n",
       "      <td id=\"T_a5ff9_row0_col3\" class=\"data row0 col3\" >There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a5ff9_row1_col0\" class=\"data row1 col0\" >Can I find information about the code's approach to handling long-running tasks and background jobs?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row1_col1\" class=\"data row1 col1\" >\n",
       "\n",
       "A:\n",
       "\n",
       "I'm not sure if this is the best way to do it, but I've found that the best way to do it is to use a background task.\n",
       "I've found that the best way to do this is to use a background task.\n",
       "I've found that the best way to do this is to use a background task.\n",
       "I've found that the best way to do this is to use a background task.\n",
       "I've found that the best way</td>\n",
       "      <td id=\"T_a5ff9_row1_col2\" class=\"data row1 col2\" >Yes, you can find information about the code's approach to handling long-running tasks and background jobs. The code is written in Python, and it uses the Python language's built-in functions for handling long-running tasks and background jobs. The code is written in Python, and it uses the Python language's built-in functions for handling long-running tasks and background jobs. The code is written in Python, and it uses the Python language's built-in functions for handling long-running</td>\n",
       "      <td id=\"T_a5ff9_row1_col3\" class=\"data row1 col3\" >Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a5ff9_row2_col0\" class=\"data row2 col0\" >How does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row2_col1\" class=\"data row2 col1\" >\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini AI is a tool for creating and managing a large-scale, distributed, and distributed-based system.\n",
       "\n",
       "Lamini AI is a tool for creating and managing a large-scale, distributed, and distributed-based system.\n",
       "\n",
       "Lamini AI is a tool for creating and managing a large-scale, distributed, and distributed-based system.\n",
       "\n",
       "Lamini AI is a tool for creating and managing a large-scale,</td>\n",
       "      <td id=\"T_a5ff9_row2_col2\" class=\"data row2 col2\" >Lamini AI is designed to handle requests for generating text that requires reasoning or decision-making based on given information. It can be used to generate text that requires reasoning or decision-making based on given information, such as text that requires reasoning or decision-making based on the context of the text. It can also be used to generate text that requires reasoning or decision-making based on the context of the text, such as text that requires reasoning or decision-making based on the context of the</td>\n",
       "      <td id=\"T_a5ff9_row2_col3\" class=\"data row2 col3\" >Lamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a5ff9_row3_col0\" class=\"data row3 col0\" >Does the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row3_col1\" class=\"data row3 col1\" >\n",
       "\n",
       "A:\n",
       "\n",
       "Yes, the `submit_job()` function exposes the following options:\n",
       "\n",
       "- `learning_rate`: The learning rate schedule.\n",
       "- `early_stopping`: Whether to stop training when the training is finished.\n",
       "\n",
       "The `learning_rate` and `early_stopping` options are available in the `train_config` and `eval_config` variables.\n",
       "\n",
       "A:\n",
       "\n",
       "Yes, the `submit_job()` function exposes</td>\n",
       "      <td id=\"T_a5ff9_row3_col2\" class=\"data row3 col2\" >Yes, the `submit_job()` function exposes advanced training options such as learning rate schedules or early stopping. However, the `submit_job()` function does not provide any advanced training options.\n",
       "\n",
       "## See also\n",
       "\n",
       "The following are the official documentation pages for the `submit_job()` function:\n",
       "\n",
       "- [API Reference](https://www.tensorflow.org/api_docs/python/tf/submit_job/)\n",
       "- [Training documentation](https://www</td>\n",
       "      <td id=\"T_a5ff9_row3_col3\" class=\"data row3 col3\" >It is unclear which `submit_job()` function is being referred to as there is no such function defined in Lamini’s python library snippets. Please provide more information or context to answer the question accurately.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a5ff9_row4_col0\" class=\"data row4 col0\" >Does the `add_data()` function support different data augmentation techniques or preprocessing options for training data?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row4_col1\" class=\"data row4 col1\" >\n",
       "\n",
       "A:\n",
       "\n",
       "The add_data() function is not designed to be used with any preprocessing or augmentation. It is designed to be used with the `train()` function.\n",
       "\n",
       "A:\n",
       "\n",
       "The add_data() function is not designed to be used with any preprocessing or augmentation. It is designed to be used with the `train()` function.\n",
       "\n",
       "The add_data() function is not designed to be used with any preprocessing or augmentation. It is designed</td>\n",
       "      <td id=\"T_a5ff9_row4_col2\" class=\"data row4 col2\" >Yes, the `add_data()` function supports different data augmentation techniques or preprocessing options for training data. The `add_data()` function can be used to generate different types of data for training. For example, the `add_data()` function can be used to generate different types of data for training. The `add_data()` function can also be used to generate different types of data for training. The `add_data()` function can also be used to generate different types of data</td>\n",
       "      <td id=\"T_a5ff9_row4_col3\" class=\"data row4 col3\" >No, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a5ff9_row5_col0\" class=\"data row5 col0\" >Can Lamini generate text for data storytelling or data visualization purposes?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row5_col1\" class=\"data row5 col1\" >\n",
       "\n",
       "Lamini is a data visualization and data storytelling tool. It is a tool that allows you to create a visual representation of your data. It is a tool that allows you to create a visual representation of your data. It is a tool that allows you to create a visual representation of your data. It is a tool that allows you to create a visual representation of your data. It is a tool that allows you to create a visual representation of your data. It is a tool that allows</td>\n",
       "      <td id=\"T_a5ff9_row5_col2\" class=\"data row5 col2\" >Yes, Lamini can generate text for data storytelling or data visualization purposes. Lamini can generate text for data storytelling or data visualization purposes by using the Lamini API. Lamini can generate text for data storytelling or data visualization purposes by using the Lamini API. Lamini can generate text for data storytelling or data visualization purposes by using the Lamini API. Lamini can generate text for data storytelling or data visualization purposes by using the Lamini API.</td>\n",
       "      <td id=\"T_a5ff9_row5_col3\" class=\"data row5 col3\" >Yes, Lamini can generate text for data storytelling or data visualization purposes using its natural language generation capabilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a5ff9_row6_col0\" class=\"data row6 col0\" >Can the documentation predict the outcome of a coin toss?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row6_col1\" class=\"data row6 col1\" >\n",
       "\n",
       "A:\n",
       "\n",
       "The answer is no.\n",
       "The coin is tossed, and the outcome is determined by the probability of heads and tails.\n",
       "The probability of heads is $1/2$, and the probability of tails is $1/2$.\n",
       "The probability of heads is $1/2$, and the probability of tails is $1/2$.\n",
       "The probability of heads is $1/2$, and the probability of tails is $1/2$.\n",
       "The probability of</td>\n",
       "      <td id=\"T_a5ff9_row6_col2\" class=\"data row6 col2\" >Yes, the documentation can predict the outcome of a coin toss. The documentation can also predict the outcome of a coin toss if the coin is biased. For example, if the coin is biased toward heads, the documentation can predict the outcome of a coin toss if the coin is biased toward heads. The documentation can also predict the outcome of a coin toss if the coin is biased toward tails. The documentation can also predict the outcome of a coin toss if the coin is biased toward tails. The documentation can</td>\n",
       "      <td id=\"T_a5ff9_row6_col3\" class=\"data row6 col3\" >I wish! This documentation only talks about Lamini’s LLM Engine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a5ff9_row7_col0\" class=\"data row7 col0\" >How does the `submit_job()` function work in Lamini? What does it mean to submit a job, and what happens behind the scenes?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row7_col1\" class=\"data row7 col1\" >\n",
       "\n",
       "The `submit_job()` function is a function that is called by the `submit()` function. The `submit()` function is a function that is called by the `submit_job()` function. The `submit_job()` function is a function that is called by the `submit()` function. The `submit()` function is a function that is called by the `submit_job()` function. The `submit()` function is a function that is called by the `submit_job()`</td>\n",
       "      <td id=\"T_a5ff9_row7_col2\" class=\"data row7 col2\" >Lamini provides a simple API for submitting jobs. It is designed to be easy to use and easy to understand. It is designed to be easy to use and easy to understand. It is designed to be easy to use and easy to understand. It is designed to be easy to use and easy to understand. It is designed to be easy to use and easy to understand. It is designed to be easy to use and easy to understand. It is designed to be easy to use and easy to</td>\n",
       "      <td id=\"T_a5ff9_row7_col3\" class=\"data row7 col3\" >The `submit_job()` function in Lamini is used to submit a job for training a machine learning model. When you submit a job, Lamini takes the training data and uses it to train a model based on the specified parameters. Behind the scenes, Lamini uses distributed computing to train the model on multiple machines, which allows for faster training times. Once the training is complete, the resulting model is saved and can be used for inference.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a5ff9_row8_col0\" class=\"data row8 col0\" >Does Lamini support generating code\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row8_col1\" class=\"data row8 col1\" >?\n",
       "\n",
       "I am trying to generate a simple HTML page using Lamini. I am using the following code:\n",
       "import lamini\n",
       "\n",
       "lamini.init()\n",
       "\n",
       "lamini.generate_page()\n",
       "\n",
       "lamini.generate_page()\n",
       "\n",
       "lamini.generate_page()\n",
       "\n",
       "lamini.generate_page()\n",
       "\n",
       "lamini.generate_page()\n",
       "\n",
       "lamini.generate_page()\n",
       "\n",
       "l</td>\n",
       "      <td id=\"T_a5ff9_row8_col2\" class=\"data row8 col2\" > for a specific task?Yes, Lamini supports generating code for specific tasks. For example, you can use Lamini to generate code for a specific task such as a specific language or a specific dataset. You can also use Lamini to generate code for a specific task such as a specific language or a specific dataset. You can also use Lamini to generate code for a specific task such as a specific language or a specific dataset. You can also use Lamini to generate</td>\n",
       "      <td id=\"T_a5ff9_row8_col3\" class=\"data row8 col3\" >Yes, Lamini supports generating code through its API.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5ff9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a5ff9_row9_col0\" class=\"data row9 col0\" >Can Lamini be used to create chatbots or virtual assistants?\n",
       " TRAIN</td>\n",
       "      <td id=\"T_a5ff9_row9_col1\" class=\"data row9 col1\" >\n",
       "\n",
       "I am a programmer and I am currently working on a project that will be used to create chatbots or virtual assistants. I am looking for a way to use Lamini to create chatbots or virtual assistants.\n",
       "\n",
       "A:\n",
       "\n",
       "Lamini is a library for creating chatbots. It is a library that is built on top of the chatbot framework.\n",
       "\n",
       "Chatbots are a type of chatbot that can be used to create a chatbot</td>\n",
       "      <td id=\"T_a5ff9_row9_col2\" class=\"data row9 col2\" >Yes, Lamini can be used to create chatbots or virtual assistants. Lamini can be used to create chatbots or virtual assistants that can be used to answer questions or provide answers to questions. Lamini can also be used to create chatbots or virtual assistants that can be used to answer questions or provide answers to questions. Lamini can also be used to create chatbots or virtual assistants that can be used to answer questions or provide answers to questions.</td>\n",
       "      <td id=\"T_a5ff9_row9_col3\" class=\"data row9 col3\" >Yes, Lamini can be used to build conversational AI agents or chatbots. It provides tools and functionalities for generating coherent and contextually appropriate responses in conversational settings, as well as support for multi-turn conversations and context-aware recommendation systems.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c930e67400>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dict_eval)\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ac868",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "381765d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ce945a0fb845d48172f85d4cb13c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5939226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "86b90256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ludyhasby/lamini_docs_instruct/commit/12cf657ca23cc03b8b7dac39746c57cd63bb7748', commit_message='Upload tokenizer', commit_description='', oid='12cf657ca23cc03b8b7dac39746c57cd63bb7748', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ludyhasby/lamini_docs_instruct', endpoint='https://huggingface.co', repo_type='model', repo_id='ludyhasby/lamini_docs_instruct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"ludyhasby/lamini_docs_instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b45fa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f521d0fb284e9c98198707fd1be8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbb6eaa70d744f8b80a63abb8ee724f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39f7eea22534a69a7d6bd6e78b4c061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fc1d15b6f94700bf0325f7011f5e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1754528738.Pongo.14384.1:   0%|          | 0.00/36.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3581768d7ea7400c9443718e5ad85207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0e5fe720064ad580654495c454f8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ludyhasby/lamini_docs_100_steps/commit/1688f3d73c19ec64cb2ec507824abd1b13a50d1e', commit_message='ludyhasby/lamini_docs_instruct', commit_description='', oid='1688f3d73c19ec64cb2ec507824abd1b13a50d1e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ludyhasby/lamini_docs_100_steps', endpoint='https://huggingface.co', repo_type='model', repo_id='ludyhasby/lamini_docs_100_steps'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"ludyhasby/lamini_docs_instruct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_9_20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
